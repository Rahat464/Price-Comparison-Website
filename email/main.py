import scrapy  # Scrape websites
from scrapy.crawler import CrawlerProcess  # Run spiders
from scrapy.exceptions import CloseSpider  # To terminate spiders if no values for price have been found

import re  # Manipulate strings
import time  # to allow program to hibernate while waiting

import mysql.connector  # Connect to database
from datetime import datetime  # Get datetime value

import smtplib  # Send Email
from email.message import EmailMessage  # Send emails w/ HTML content

# Establish connection with local MySQL server
db = mysql.connector.connect(
    host="localhost",
    user="root",
    passwd="root",
    database="database",
)
mycursor = db.cursor()
db.autocommit = True

# email HTML
html = """
<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Work+Sans&display=swap">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <style>
      /*H1 Text alignment*/
      h1{
          color:black;
          text-align: center;
          padding: 75px;
      }

      /*GOOGLE FONT*/
      body {
          font-family: 'Work Sans', sans-serif;
          font-size: 36px;
      }

      .centre{
      display: block;
      margin-left: auto;
      margin-right: auto;
      }

      .bottom {
        bottom: 0;
      }
    </style>
    
</head>
<body style="background: rgb(127,127,213);background: linear-gradient(90deg, rgba(127,127,213,1) 0%, rgba(134,168,231,1) 50%, rgba(145,234,228,1) 100%);">
    <header>
        <div class="navbar_div">
            <ul class="navbar">
                <img class="centre" src="src_navbar_logo" alt="logo"> <!--IMAGE SRC-->
            </ul>
        </div>

        <img class="centre product_image" src="src_product_image" alt="Product Image" height:> <!--IMAGE SRC-->
        <br>
        <a class="centre" href="product_link"  style="width: 50%"><h2>product_name</h2></a> <!--PRODUCT NAME-->
        <br>
        <h2 class="centre" style="width: 50%">The price has dropped from £original_price to £current_price. That's a percentage% reduction!</h2>
        <p class="centre" style="width: 50%"> This email has been automatically generated by Price Comparison Website. You have received this email because you have consented to receive emails.</p>
        <br>
    </header>
</body>
</html>
"""


# GET scrapy scraping settings
def scrapy_settings():
    settings = dict()
    settings["ROBOTSTXT_OBEY"] = False


class AmazonSpider(scrapy.Spider):
    name = 'amazon'

    # Initiate instance to allow arguments to pass through scrapy spider
    def __init__(self, url="", **kwargs):
        self.allowed_domains = ['amazon.co.uk/']
        self.start_urls = [url]
        super().__init__(**kwargs)

    def parse(self, response, **kwargs):
        items = dict(asin=None, name=None, price=None, imageurl=None, detailpageurl=None, stock=None)

        #  Define XPATH/CSS selectors to fetch correct data
        temp_index = self.start_urls[0].find("/dp/") + 4  # Find where the ASIN code starts in the product link
        items["asin"] = self.start_urls[0][temp_index:temp_index+10]  # Get the asin link

        items["name"] = response.xpath('//*[(@id = "title")]').css("#productTitle::text").extract()
        items["name"] = str(items["name"])[2:-2].strip()  # Remove bracket and quotation marks

        items["price"] = response.xpath('//*[(@id = "corePrice_feature_div")]//*[contains(concat( " ", @class, " " ),'
                                        'concat( " ", "a-price-whole", " " ))]').css(".a-price-whole::text").extract()
        items["price"] = str(items["price"])[2:-2]  # Remove bracket and quotation marks

        items["imageurl"] = response.css("#imgTagWrapperId img").xpath("@src").extract()
        items["imageurl"] = str(items["imageurl"])[2:-2]  # Remove bracket and quotation marks

        items["detailpageurl"] = self.start_urls

        # Determine if there is stock for the website
        # If price has been scraped, it means the product is available and there is stock
        # If price has NOT been scraped, it means the product is NOT available and there is NO stock
        if len(items["price"]) != 0:  # Check if the price has been
            items["stock"] = True
        else:  # If price value does not exist,
            items["stock"] = False

        # Stops the spider and SQL commit if there is no value for price.
        if items["price"] == "" or items["price"] is None:
            raise CloseSpider('No Price found')
        else:
            mycursor.execute(
                "INSERT INTO search_amazon (asin, name, price, image_link, product_link, datetime, stock)"
                "VALUES (%s, %s, %s, %s, %s, %s, %s)",
                (items["asin"],
                 str(items["name"]),
                 str(items["price"])[:64],
                 str(items["imageurl"]),
                 str(items["detailpageurl"][0]),
                 datetime.now(),
                 items["stock"])
            )
            db.commit()

        yield items


class EbaySpider(scrapy.Spider):
    name = 'ebay'

    def __init__(self, url="", **kwargs):
        self.allowed_domains = ['ebay.co.uk/']
        self.start_urls = [url]
        super().__init__(**kwargs)

    def parse(self, response, **kwargs):
        items = dict(name=None, price=None, imageurl=None, detailpageurl=None, stock=None)

        items["name"] = response.xpath('//*[(@id = "itemTitle")]').css("#itemTitle::text").extract()
        items["price"] = response.xpath('//*[(@id = "prcIsum")]').css("#prcIsum::text").extract()
        items["imageurl"] = response.css("#icImg").xpath('@src').extract()
        items["detailpageurl"] = self.start_urls

        if len(items["price"]) != 0:
            items["stock"] = True
        else:
            items["stock"] = False

        # Stops the spider and SQL commit if there is no value for price.
        if items["price"] == "" or items["price"] is None:
            raise CloseSpider('No Price found')
        else:
            mycursor.execute(
                "INSERT INTO search_ebay (product_link, name, price, image_link, datetime, stock)"
                "VALUES (%s, %s, %s, %s, %s, %s)",
                (str(items["detailpageurl"])[2:700],
                 str(items["name"][0])[:80],
                 str(items["price"][0])[:16 ],
                 str(items["imageurl"][0])[:512],
                 datetime.now(),
                 items["stock"])
            )
            db.commit()

        yield items


class NeweggSpider(scrapy.Spider):
    name = 'newegg'

    def __init__(self, url="", **kwargs):
        self.allowed_domains = ['newegg.com/global/uk']
        self.start_urls = [url]
        super().__init__(**kwargs)

    def parse(self, response, **kwargs):
        items = dict(name=None, price=None, imageurl=None, detailpageurl=None, stock=None)

        items["name"] = response.xpath('//*[contains(concat( " ", @class, " " ), concat( " ", "product-title", " " ))]'
                                       ).css(".product-title::text").extract()
        items["price"] = response.css(".price-current strong::text").extract()
        items["imageurl"] = response.css(".product-view-img-original").xpath('@src').extract()
        items["detailpageurl"] = self.start_urls
        items["stock"] = False if response.css(".product-inventory strong::text") == " OUT OF STOCK." else True

        # Stops the spider and SQL commit if there is no value for price.
        if items["price"] == "" or items["price"] is None:
            raise CloseSpider('No Price found')
        else:
            mycursor.execute(
                "INSERT INTO search_newegg (product_link, name, price, image_link, datetime, stock)"
                "VALUES (%s, %s, %s, %s, %s, %s)",
                (str(items["detailpageurl"][0])[:512],
                 str(items["name"][0][:512]),
                 str(items["price"][0])[:16],
                 str(items["imageurl"][0])[:512],
                 datetime.now(),
                 items["stock"])
                )
            db.commit()

        yield items


def price_drop(url):

    def price_regex(row):

        try:
            if results[row][1] == "" or results[row][1] is None:
                return
        except IndexError:
            return

        index = str(results[row][1]).find(".")

        # Removes alphanumeric numbers
        if index == -1:  # If . has not been found
            return re.sub("[^0-9]", "", str(results[row][1]))
        else:  # If . has been found
            return re.sub("[^0-9]", "", str(results[row][1][:-(index + 3)]))

    def send_email(retailer):

        #  Composing Email
        message = EmailMessage()
        message.set_content("Please enable HTML! You will not be able to view this email without it.")

        mycursor.execute('SELECT DISTINCT email FROM price_alert_user_email WHERE product_link = "{}"'.format(url))
        message["To"] = mycursor.fetchall()

        # Fetch Product name
        mycursor.execute("SELECT DISTINCT name, product_link, image_link FROM search_{} WHERE product_link = {}"
                         .format(retailer, url))
        product = mycursor.fetchone()  # 0 = Name, 1 = ProductLink, 2 = Image link
        message["Subject"] = "Price Alert Website: '{}...' is now cheaper!".format(product[0][:25])

        message.add_alternative(html.replace("src_navbar_logo", "https://i.imgur.com/aB3gEVl.png")
                                .replace("src_product_image", product[2])
                                .replace("product_link", product[1])
                                .replace("product_name", product[0])
                                .replace("original_price", previous_price)
                                .replace("current_price", current_price)
                                .replace("percentage", 100-(current_price/previous_price)*100),
                                subtype="html")

        server.send_message(message)
        server.quit()
        print("Emails Sent to:", message["To"])

    # Check if email is of Amazon/Ebay/Newegg
    select_statement = 'SELECT name, price, product_link, image_link, datetime FROM search_{} WHERE {} = "{}" ' \
                       'ORDER BY datetime DESC'

    if "https://www.amazon.co.uk/" in str(url):
        asin = url.find("/dp/") + 4
        mycursor.execute(select_statement.format("amazon", "asin", url[asin:asin+10]))
        results = mycursor.fetchall()

        current_price = price_regex(0)
        previous_price = price_regex(1)

        if current_price is None or previous_price is None:
            return

        # Make values integers
        current_price = int(price_regex(0))
        previous_price = int(price_regex(1))

        if current_price < previous_price:
            send_email("amazon")

    elif "https://www.ebay.co.uk/" in str(url):
        mycursor.execute(select_statement.format("ebay", "product_link", url))
        results = mycursor.fetchall()

        current_price = price_regex(0)
        previous_price = price_regex(1)

        if current_price or previous_price is None:
            return

        # Check if price has dropped
        if current_price < previous_price:
            send_email("ebay")

    elif "https://www.newegg.com/global/uk-en/" in str(url):
        mycursor.execute(select_statement.format("newegg", "product_link", url))
        results = mycursor.fetchall()

        current_price = int(price_regex(0))
        previous_price = int(price_regex(1))

        # Check if price has dropped
        if current_price < previous_price:
            send_email("newegg")


def crawl():
    process = CrawlerProcess(scrapy_settings())
    mycursor.execute("SELECT DISTINCT product_link FROM price_alert_user_email")
    product_links = mycursor.fetchall()
    print(product_links)

    for link in product_links:
        try:
            if len(str(link)) < 6:
                continue
        except TypeError:
            continue

        link = str(link)[2:-3]

        if "https://www.amazon.co.uk/" in str(link):
            process.crawl(AmazonSpider, url=str(link))

        elif "https://www.ebay.co.uk/" in str(link):
            process.crawl(EbaySpider, url=str(link))

        elif "https://www.newegg.com/global/uk-en/" in str(link):
            process.crawl(NeweggSpider, url=str(link))

    # process.start()

    for product_link in product_links:
        try:
            if len(str(product_link)) < 6:
                continue
        except TypeError:
            continue

        link = str(product_link)[2:-3]
        price_drop(link)


while True:
    crawl()
    print("Sleeping...")
    time.sleep(86400)
